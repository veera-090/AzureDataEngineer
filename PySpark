# type(spark)
# dir(spark)
# help(spark)
# help(spark.createDataFrame)
from pyspark.sql import *
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
help(StructField)
help(StructType)

from pyspark.sql.functions import col, lit, expr, column
from pyspark.sql.functions import desc, asc

# ---------------------------------------------

# different ways to create dataframes:

# 1st method:

data = [(1, 'Radha', 90000), (2, 'Krishna', 700000), (3, 'Vrindha', 60000)]
schema = ['id', 'name', 'salary']

df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()

# -------------

# 2nd method:

from pyspark.sql.types import StructField, StructType, StringType, IntegerType

data = [(1, 'Radha', 90000), (2, 'Krishna', 700000), (3, 'Vrindha', 60000)]
schema = StructType([StructField(name='id', dataType=IntegerType()),\
                    StructField(name='name', dataType=StringType()),\
                    StructField(name='salary', dataType=IntegerType())])

df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()
display(df)

# -------------

# 3rd method:

from pyspark.sql.types import StructField, StructType, StringType, IntegerType

data = [(1, ('Radha', 'Krishna'), 90000), (2, ('Madhan', 'Mohan'), 700000), (3, ('Vrindha', 'Vihari'), 60000)]

nameSchema = StructType([StructField(name='firstName', dataType=StringType()),\
                        StructField(name='lastName', dataType=StringType())])

schema = StructType([StructField(name='id', dataType=IntegerType()),\
                    StructField(name='name', dataType=nameSchema),\
                    StructField(name='salary', dataType=IntegerType())])

df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()
display(df)


# -------------

# 4th method:

from pyspark.sql.types import StructField, StructType, StringType, IntegerType
data = [(1, 'Radha', 90000), (2, 'Krishna', 700000), (3, 'Vrindha', 60000)]

schema = StructType().add('id', IntegerType()).add('name', StringType()).add('salary', IntegerType())
df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()
display(df)


# -------------

# 5th method:

from pyspark.sql.types import StructField, StructType, StringType, IntegerType

data = [(1, ('Radha', 'Krishna'), 90000), (2, ('Madhan', 'Mohan'), 700000), (3, ('Vrindha', 'Vihari'), 60000)]
nameForm = StructType().add('firstName', StringType()).add('lastName', StringType())

schema = StructType().add('id', IntegerType()).add('name', nameForm).add('salary', IntegerType())
df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()
display(df)

# ---------------------------------------------


# different ways to read CSV file data into data frames:

# 1st method:
df1 = spark.read.csv('dbfs:/FileStore/shared_uploads/nityaveera6@gmail.com/Order.csv', header=True)
df1.show(4)

# 2nd method:
df2 = spark.read.format(source='csv').option(key='header', value=True).load(path='dbfs:/FileStore/shared_uploads/nityaveera6@gmail.com/Order.csv')
df2.show(5)

# 3rd method:
df3 = spark.read.format(source='csv').options(header=True).load('dbfs:/FileStore/shared_uploads/nityaveera6@gmail.com/Order.csv')
df3.show(4)

# 4th method:
df = spark.read.csv(['dbfs:/FileStore/shared_uploads/Temp_Practice1/Read_Data/order1.csv', 'dbfs:/FileStore/shared_uploads/Temp_Practice1/Order.csv'], header=True)
df.show()

# ---------------------------------------------

# different ways to read JSON file data into data frames:

# 1st method:
df = spark.read.json('dbfs:/FileStore/shared_uploads/Temp/read/*.json')
df.show()

# 2nd method:
df2 = spark.read.format(source='json').option(key='header', value=True).load(path='dbfs:/FileStore/shared_uploads/Temp/read/*.json')
df2.show()

# 3rd method:
df3 = spark.read.format(source='json').options(header=True).load(path='dbfs:/FileStore/shared_uploads/Temp/read/*.json')
df3.show(4)


# ---------------------------------------------

# different modes to write CSV file data into data frames:

df.write.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True, mode='overwrite')
df.write.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True, mode='error')
df.write.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True, mode='ignore')
df.write.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True, mode='append')

display(spark.read.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True))

df1 = spark.read.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True)
# df1.show()
display(df1)


# ---------------------------------------------

# different modes to write JSON file data into data frames:

df.write.json(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order', mode='overwrite')
df.write.json(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order', mode='error')
df.write.json(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order', mode='ignore')
df.write.json(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order', mode='append')

display(spark.read.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order'))

df1 = spark.read.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order')
df1.show()
display(df1)


# ---------------------------------------------

# ArrayType in a data frame:

from pyspark.sql.types import StructField, StructType, StringType, IntegerType, ArrayType
data = [(1, ['Radha', 'Krishna'], 90000), (2, ['Vrindha', 'Vihari'], 80000), (3, ['Madhan', 'Mohan'], 70000)]

schema = ['id', 'name', 'salary']

# schema = StructType([StructField(name='id', dataType=IntegerType()),\
#                     StructField(name='name', dataType=ArrayType(StringType())),\
#                     StructField(name='salary', dataType=IntegerType())])

df = spark.createDataFrame(data=data, schema=schema) 
df.show()
df.printSchema()
# display(df)

# accessing the index values:

df1 = df.withColumn('firstName', df.name[0])
df1.show()

df2 = df.withColumn('firstName', df.name[0]).withColumn('lastName', df.name[1])
df2.show()

# Result:

+---+-----------------+------+
| id|             name|salary|
+---+-----------------+------+
|  1| [Radha, Krishna]| 90000|
|  2|[Vrindha, Vihari]| 80000|
|  3|  [Madhan, Mohan]| 70000|
+---+-----------------+------+

+---+-----------------+------+---------+
| id|             name|salary|firstName|
+---+-----------------+------+---------+
|  1| [Radha, Krishna]| 90000|    Radha|
|  2|[Vrindha, Vihari]| 80000|  Vrindha|
|  3|  [Madhan, Mohan]| 70000|   Madhan|
+---+-----------------+------+---------+

+---+-----------------+------+---------+--------+
| id|             name|salary|firstName|lastName|
+---+-----------------+------+---------+--------+
|  1| [Radha, Krishna]| 90000|    Radha| Krishna|
|  2|[Vrindha, Vihari]| 80000|  Vrindha|  Vihari|
|  3|  [Madhan, Mohan]| 70000|   Madhan|   Mohan|
+---+-----------------+------+---------+--------+


# ---------------------------------------------

# array() in a data frame:

from pyspark.sql.functions import array

data = [(1,2), (3,4)]
schema = ['num1', 'num2']

df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()
display(df)

# converting the numbers into the array:
df2 = df.withColumn('numbers', array(df.num1, df.num2)).show()


# Result:

+----+----+
|num1|num2|
+----+----+
|   1|   2|
|   3|   4|
+----+----+

+----+----+-------+
|num1|num2|numbers|
+----+----+-------+
|   1|   2| [1, 2]|
|   3|   4| [3, 4]|
+----+----+-------+


# ---------------------------------------------

# explode():

from pyspark.sql.functions import explode, col
data = [(1,  'Radha', ['Java', 'Python'], 90000), (2, 'Vrindha', ['Scala', 'R'], 80000), (3, 'Madhan', ['Azure', 'AWS'], 70000)]

schema = ['id', 'name', 'Skills', 'salary']

df = spark.createDataFrame(data=data, schema=schema) 
df.show()
# df.printSchema()
# display(df)

df1 = df.withColumn('New_Skills', explode(df.Skills))
# df1 = df.withColumn('New_Skills', explode(col('skills')))
df1.show()

# Results:

+---+-------+--------------+------+
| id|   name|        Skills|salary|
+---+-------+--------------+------+
|  1|  Radha|[Java, Python]| 90000|
|  2|Vrindha|    [Scala, R]| 80000|
|  3| Madhan|  [Azure, AWS]| 70000|
+---+-------+--------------+------+

+---+-------+--------------+------+----------+
| id|   name|        Skills|salary|New_Skills|
+---+-------+--------------+------+----------+
|  1|  Radha|[Java, Python]| 90000|      Java|
|  1|  Radha|[Java, Python]| 90000|    Python|
|  2|Vrindha|    [Scala, R]| 80000|     Scala|
|  2|Vrindha|    [Scala, R]| 80000|         R|
|  3| Madhan|  [Azure, AWS]| 70000|     Azure|
|  3| Madhan|  [Azure, AWS]| 70000|       AWS|
+---+-------+--------------+------+----------+


# ---------------------------------------------

# split():

from pyspark.sql.functions import split, col
data = [(1,  'Radha', 'Java, Python', 90000), (2, 'Vrindha', 'Scala, R', 80000), (3, 'Madhan', 'Azure, AWS', 70000)]

schema = ['id', 'name', 'Skills', 'salary']

df = spark.createDataFrame(data=data, schema=schema) 
df.show()
# df.printSchema()
# display(df)

# df1 = df.withColumn('New_Skills', split(df.Skills, ','))
df1 = df.withColumn('New_Skills', split(col('skills'), ','))

df1.show()

# Results:

+---+-------+-----------+------+
| id|   name|     Skills|salary|
+---+-------+-----------+------+
|  1|  Radha|Java,Python| 90000|
|  2|Vrindha|    Scala,R| 80000|
|  3| Madhan|  Azure,AWS| 70000|
+---+-------+-----------+------+

+---+-------+-----------+------+--------------+
| id|   name|     Skills|salary|    New_Skills|
+---+-------+-----------+------+--------------+
|  1|  Radha|Java,Python| 90000|[Java, Python]|
|  2|Vrindha|    Scala,R| 80000|    [Scala, R]|
|  3| Madhan|  Azure,AWS| 70000|  [Azure, AWS]|
+---+-------+-----------+------+--------------+


# ---------------------------------------------

# array():

from pyspark.sql.functions import array, col

data = [(1,  'Radha', 'Java', 'Python'), (2, 'Vrindha', 'Scala', 'R'), (3, 'Madhan', 'Azure', 'AWS')]

schema = ['id', 'name', 'firstSkill', 'secondSkill']

df = spark.createDataFrame(data=data, schema=schema) 
df.show()
# df.printSchema()

# df1 = df.withColumn('Skills', array(df.firstSkill , df.secondSkill))
df1 = df.withColumn('Skills', array(col('firstSkill'), col('secondSkill')))
df1.show()

# Ressults:

+---+-------+----------+-----------+
| id|   name|firstSkill|secondSkill|
+---+-------+----------+-----------+
|  1|  Radha|      Java|     Python|
|  2|Vrindha|     Scala|          R|
|  3| Madhan|     Azure|        AWS|
+---+-------+----------+-----------+

+---+-------+----------+-----------+--------------+
| id|   name|firstSkill|secondSkill|        Skills|
+---+-------+----------+-----------+--------------+
|  1|  Radha|      Java|     Python|[Java, Python]|
|  2|Vrindha|     Scala|          R|    [Scala, R]|
|  3| Madhan|     Azure|        AWS|  [Azure, AWS]|
+---+-------+----------+-----------+--------------+


# ---------------------------------------------

# array_contains():

from pyspark.sql.functions import array_contains, col

data = [(1, 'Virndha', ['java', '.net']), (2, 'Vihari', ['python', 'scala']), (3, 'Vaibhav', ['Azure', 'AWS'])]

schema = ['id', 'name', 'skills']

df = spark.createDataFrame(data=data, schema=schema) 
df.show()
df.printSchema()

# df1 = df.withColumn('is Java present ?', array_contains(df.skills, 'java'))
df1 = df.withColumn('is Python present ?', array_contains(col('skills'), 'python'))
df1.show()


# Results:

+---+-------+---------------+
| id|   name|         skills|
+---+-------+---------------+
|  1|Virndha|   [java, .net]|
|  2| Vihari|[python, scala]|
|  3|Vaibhav|   [Azure, AWS]|
+---+-------+---------------+

+---+-------+---------------+-------------------+
| id|   name|         skills|is Python present ?|
+---+-------+---------------+-------------------+
|  1|Virndha|   [java, .net]|              false|
|  2| Vihari|[python, scala]|               true|
|  3|Vaibhav|   [Azure, AWS]|              false|
+---+-------+---------------+-------------------+


# ---------------------------------------------
# MapType
# map_keys()
# map_values()

from pyspark.sql.types import MapType, StructType, StructField, StringType, IntegerType

data =[('Krishna', {'hair':'black', 'eyes':'brown'}), ('Radha', {'hair':'black', 'eyes':'blue'})]

# schema = ['name', 'properties']

schema = StructType([StructField(name='name', dataType=StringType()),\
                    StructField(name='properties', dataType=MapType(StringType(), StringType()))])

df = spark.createDataFrame(data= data, schema=schema)
df.show(truncate=False)
# df.printSchema()

# MapType():
df1 = df.withColumn('header', df.properties['hair'])
df1.show()

# Access the keys or values:
df4 = df.withColumn('access_Keys', df.properties.getItem('keys'))
df4.show(truncate=False)

# map_keys():
from pyspark.sql.functions import map_keys
df2 = df.withColumn('keys', map_keys(df.properties))
df2.show(truncate=False)

# mpa_values():
from pyspark.sql.functions import map_values
df3 = df.withColumn('values', map_values(df.properties))
df3.show(truncate=False)

# Results:

+-------+------------------------------+
|name   |properties                    |
+-------+------------------------------+
|Krishna|{eyes -> brown, hair -> black}|
|Radha  |{eyes -> blue, hair -> black} |
+-------+------------------------------+

+-------+------------------------------+------+
|name   |properties                    |header|
+-------+------------------------------+------+
|Krishna|{eyes -> brown, hair -> black}|black |
|Radha  |{eyes -> blue, hair -> black} |black |
+-------+------------------------------+------+

+-------+------------------------------+-----------+
|name   |properties                    |access_Keys|
+-------+------------------------------+-----------+
|Krishna|{eyes -> brown, hair -> black}|null       |
|Radha  |{eyes -> blue, hair -> black} |null       |
+-------+------------------------------+-----------+

+-------+------------------------------+------------+
|name   |properties                    |keys        |
+-------+------------------------------+------------+
|Krishna|{eyes -> brown, hair -> black}|[eyes, hair]|
|Radha  |{eyes -> blue, hair -> black} |[eyes, hair]|
+-------+------------------------------+------------+

+-------+------------------------------+--------------+
|name   |properties                    |values        |
+-------+------------------------------+--------------+
|Krishna|{eyes -> brown, hair -> black}|[brown, black]|
|Radha  |{eyes -> blue, hair -> black} |[blue, black] |
+-------+------------------------------+--------------+


# ---------------------------------------------


# how to create a data frame using the Row class:

from pyspark.sql import Row

# method-1:
row1 = Row(id=1, name='Madhan', salary=70000)
row2 = Row(id=2, name='Mohan', salary=60000)

print(row1.name + ' ' + str(row2.salary))

data = [row1, row2]
df = spark.createDataFrame(data)
df.show()

# method:2
row1 = Row(1, 'Krishna', 80000)
row2 = Row(2, 'Radha', 90000)

print(row1[1] + ' ' + str(row1[2]))

data = [row1, row2]
df = spark.createDataFrame(data=data)
df.show()


# method:3
person = Row('id', 'name', 'salary')
r1 = person(1, 'Vrinda', 80000)
r2 = person(2, 'Vihari', 70000)
data = [r1, r2]

print(r1.name + ' ' + r2.name)

df = spark.createDataFrame(data)
df.show()


Madhan 60000
+---+------+------+
| id|  name|salary|
+---+------+------+
|  1|Madhan| 70000|
|  2| Mohan| 60000|
+---+------+------+

Krishna 80000
+---+-------+-----+
| _1|     _2|   _3|
+---+-------+-----+
|  1|Krishna|80000|
|  2|  Radha|90000|
+---+-------+-----+

Vrinda Vihari
+---+------+------+
| id|  name|salary|
+---+------+------+
|  1|Vrinda| 80000|
|  2|Vihari| 70000|
+---+------+------+


# ---------------------------------------------

# column:


from pyspark.sql.functions import lit

col1 = lit('India')
print(type(col1))

data = [(1, 'Krishna', 'Male', 80000), (2, 'Radha', 'Female', 90000)]

schema = ['id', 'name', 'gender', 'salary']

df = spark.createDataFrame(data=data, schema=schema)
df.show()

df1 = df.withColumn('Country', lit('India'))
df1.show()

# Results:

<class 'pyspark.sql.column.Column'>
+---+-------+------+------+
| id|   name|gender|salary|
+---+-------+------+------+
|  1|Krishna|  Male| 80000|
|  2|  Radha|Female| 90000|
+---+-------+------+------+

+---+-------+------+------+-------+
| id|   name|gender|salary|Country|
+---+-------+------+------+-------+
|  1|Krishna|  Male| 80000|  India|
|  2|  Radha|Female| 90000|  India|
+---+-------+------+------+-------+

root
 |-- id: long (nullable = true)
 |-- name: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: long (nullable = true)
 |-- Country: string (nullable = false)


# ---------------------------------------------

# different ways to get/access the columns:

from pyspark.sql.functions import col

df1.select(df.gender).show()
df1.select(df['gender']).show()
df.select(col('gender')).show()


# Results:

+------+
|gender|
+------+
|  Male|
|Female|
+------+


# ---------------------------------------------

from pyspark.sql.types import StructType, StructField, StringType, IntegerType
data = [(1, 'Krishna', 'Male', 80000, ('black', 'blue')), (2, 'Radha', 'Female', 90000, ('black', 'brown'))]
# schema = ['id', 'name', 'gender', 'salary', 'properties']

propType = StructType([StructField(name='hair', dataType=StringType()),\
                    StructField(name='eyes', dataType=StringType())])

schema = StructType([StructField(name='id', dataType=IntegerType()),\
                    StructField(name='name', dataType=StringType()),\
                    StructField(name='gender', dataType=StringType()),\
                    StructField(name='salary', dataType=IntegerType()),\
                    StructField(name='propType', dataType=propType)])

df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()

# different ways to access Struct Columns:
from pyspark.sql.functions import col

df1 = df.select(df.propType.hair).show()
df2 = df.select(df['propType.eyes']).show()
df3 = df.select(col('propType.hair')).show()
df4 = df.select(df.propType.getItem('eyes')).show()


# Results:

+---+-------+------+------+--------------+
| id|   name|gender|salary|      propType|
+---+-------+------+------+--------------+
|  1|Krishna|  Male| 80000| {black, blue}|
|  2|  Radha|Female| 90000|{black, brown}|
+---+-------+------+------+--------------+

root
 |-- id: integer (nullable = true)
 |-- name: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)
 |-- propType: struct (nullable = true)
 |    |-- hair: string (nullable = true)
 |    |-- eyes: string (nullable = true)

+-------------+
|propType.hair|
+-------------+
|        black|
|        black|
+-------------+

+-----+
| eyes|
+-----+
| blue|
|brown|
+-----+

+-----+
| hair|
+-----+
|black|
|black|
+-----+

+-------------+
|propType.eyes|
+-------------+
|         blue|
|        brown|
+-------------+


# ---------------------------------------------

# when() & otherwise():

data = [(1, 'Krishna', 'M', 140000), (2, 'Radha', 'F', 160000), (3, 'Vrinda', '', 150000)]
schema = ['id', 'name', 'gender', 'salary']

df = spark.createDataFrame(data=data, schema=schema)
df.show()

df1 = df.select(df.id, df.name, when(df.gender == 'M', 'Male').when(df.gender == 'F', 'Female').otherwise('Unknown').alias('Gender'), df.salary)
df1.show()

# Results:

+---+-------+------+------+
| id|   name|gender|salary|
+---+-------+------+------+
|  1|Krishna|     M|140000|
|  2|  Radha|     F|160000|
|  3| Vrinda|      |150000|
+---+-------+------+------+

+---+-------+-------+------+
| id|   name| Gender|salary|
+---+-------+-------+------+
|  1|Krishna|   Male|140000|
|  2|  Radha| Female|160000|
|  3| Vrinda|Unknown|150000|
+---+-------+-------+------+

# ---------------------------------------------

# alias(), case(), asc(), desc(), like(), filter(), when()

data = [(1, 'Krishna', 140000), (2, 'Radha', 160000), (3, 'Vrinda', 150000), (4, 'Vrihari', 130000)]
schema = ['id', 'name', 'salary']

df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()

df1 = df.select(df.id.alias('emp_id'), df.name.alias('empp_name'), df.salary.alias('emp_salary')).show()
# df2 = df.sort(df.name.desc()).show()
# df3 = df.sort(df.salary.desc()).show()
# df4 = df.filter(df.name.like('vr%')).show()
# df5 = df.filter(df.name.like('_r%')).show()
# df6 = df.where(df.name == 'vrinda').show()
# df7 = df.where("name == 'vrihari'").show()
# df8 = df.filter(df.name == 'Krishna').show()
# df9 = df.filter((df.name == 'Radha') & (df.salary == 160000)).show()
df10 = df.select(df.id, df.name, df.salary.cast('int'))
df10.show()
df10.printSchema()


# Results:

+---+-------+------+
| id|   name|salary|
+---+-------+------+
|  1|Krishna|140000|
|  2|  Radha|160000|
|  3| Vrinda|150000|
|  4|Vrihari|130000|
+---+-------+------+

root
 |-- id: long (nullable = true)
 |-- name: string (nullable = true)
 |-- salary: long (nullable = true)

+------+---------+----------+
|emp_id|empp_name|emp_salary|
+------+---------+----------+
|     1|  Krishna|    140000|
|     2|    Radha|    160000|
|     3|   Vrinda|    150000|
|     4|  Vrihari|    130000|
+------+---------+----------+

+---+-------+------+
| id|   name|salary|
+---+-------+------+
|  1|Krishna|140000|
|  2|  Radha|160000|
|  3| Vrinda|150000|
|  4|Vrihari|130000|
+---+-------+------+

root
 |-- id: long (nullable = true)
 |-- name: string (nullable = true)
 |-- salary: integer (nullable = true)


# ---------------------------------------------

# distinct(), dropDuplicates():


# help(df.distinct)
# help(df.dropDuplicates)

data = [(1, 'Krishna', 'Male', 80000), (2, 'Radha', 'Female', 90000), (3, 'Vrinda', 'Female', 130000), (4, 'Radha', 'Female', 140000), (3, 'Vrinda', 'Female', 130000)]
schema = ['id', 'name', 'gender', 'salary']

df = spark.createDataFrame(data=data, schema=schema)
# df.show()

df.distinct().show()
df.dropDuplicates(['name','gender']).show()
df.dropDuplicates(['gender']).show()
df.dropDuplicates(['gender']).count()

# Results:

+---+-------+------+------+
| id|   name|gender|salary|
+---+-------+------+------+
|  1|Krishna|  Male| 80000|
|  2|  Radha|Female| 90000|
|  3| Vrinda|Female|130000|
|  4|  Radha|Female|140000|
+---+-------+------+------+

+---+-------+------+------+
| id|   name|gender|salary|
+---+-------+------+------+
|  1|Krishna|  Male| 80000|
|  2|  Radha|Female| 90000|
|  3| Vrinda|Female|130000|
+---+-------+------+------+

+---+-------+------+------+
| id|   name|gender|salary|
+---+-------+------+------+
|  2|  Radha|Female| 90000|
|  1|Krishna|  Male| 80000|
+---+-------+------+------+

Out[17]: 2


# ---------------------------------------------

# sort(), orderBy():


data = [(1, 'Krishna', 'Male', 80000, 'HR'), (2, 'Radha', 'Female', 90000, 'IT'), (3, 'Vrinda', 'Female', 130000, 'IT'), (4, 'Radha', 'Female', 140000, 'HR'), (5, 'Vrinda', 'Female', 130000, 'HR')]
schema = ['id', 'name', 'gender', 'salary', 'dept']

df = spark.createDataFrame(data=data, schema=schema)
df.show()

df.sort(df.dept.desc()).show()
df.orderBy(df.name.desc()).show()

# Results:

+---+-------+------+------+----+
| id|   name|gender|salary|dept|
+---+-------+------+------+----+
|  1|Krishna|  Male| 80000|  HR|
|  2|  Radha|Female| 90000|  IT|
|  3| Vrinda|Female|130000|  IT|
|  4|  Radha|Female|140000|  HR|
|  5| Vrinda|Female|130000|  HR|
+---+-------+------+------+----+

+---+-------+------+------+----+
| id|   name|gender|salary|dept|
+---+-------+------+------+----+
|  2|  Radha|Female| 90000|  IT|
|  3| Vrinda|Female|130000|  IT|
|  4|  Radha|Female|140000|  HR|
|  1|Krishna|  Male| 80000|  HR|
|  5| Vrinda|Female|130000|  HR|
+---+-------+------+------+----+

+---+-------+------+------+----+
| id|   name|gender|salary|dept|
+---+-------+------+------+----+
|  3| Vrinda|Female|130000|  IT|
|  5| Vrinda|Female|130000|  HR|
|  2|  Radha|Female| 90000|  IT|
|  4|  Radha|Female|140000|  HR|
|  1|Krishna|  Male| 80000|  HR|
+---+-------+------+------+----+

# ---------------------------------------------


