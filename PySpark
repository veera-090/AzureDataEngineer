# type(spark)
# dir(spark)
# help(spark)
# help(spark.createDataFrame)
from pyspark.sql import *
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
help(StructField)
help(StructType)

from pyspark.sql.functions import col, lit, expr, column
from pyspark.sql.functions import desc, asc

# ---------------------------------------------

# different ways to create dataframes:

# 1st method:

data = [(1, 'Radha', 90000), (2, 'Krishna', 700000), (3, 'Vrindha', 60000)]
schema = ['id', 'name', 'salary']

df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()

# -------------

# 2nd method:

from pyspark.sql.types import StructField, StructType, StringType, IntegerType

data = [(1, 'Radha', 90000), (2, 'Krishna', 700000), (3, 'Vrindha', 60000)]
schema = StructType([StructField(name='id', dataType=IntegerType()),\
                    StructField(name='name', dataType=StringType()),\
                    StructField(name='salary', dataType=IntegerType())])

df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()
display(df)

# -------------

# 3rd method:

from pyspark.sql.types import StructField, StructType, StringType, IntegerType

data = [(1, ('Radha', 'Krishna'), 90000), (2, ('Madhan', 'Mohan'), 700000), (3, ('Vrindha', 'Vihari'), 60000)]

nameSchema = StructType([StructField(name='firstName', dataType=StringType()),\
                        StructField(name='lastName', dataType=StringType())])

schema = StructType([StructField(name='id', dataType=IntegerType()),\
                    StructField(name='name', dataType=nameSchema),\
                    StructField(name='salary', dataType=IntegerType())])

df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()
display(df)


# -------------

# 4th method:

from pyspark.sql.types import StructField, StructType, StringType, IntegerType
data = [(1, 'Radha', 90000), (2, 'Krishna', 700000), (3, 'Vrindha', 60000)]

schema = StructType().add('id', IntegerType()).add('name', StringType()).add('salary', IntegerType())
df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()
display(df)


# -------------

# 5th method:

from pyspark.sql.types import StructField, StructType, StringType, IntegerType

data = [(1, ('Radha', 'Krishna'), 90000), (2, ('Madhan', 'Mohan'), 700000), (3, ('Vrindha', 'Vihari'), 60000)]
nameForm = StructType().add('firstName', StringType()).add('lastName', StringType())

schema = StructType().add('id', IntegerType()).add('name', nameForm).add('salary', IntegerType())
df = spark.createDataFrame(data=data, schema=schema)
df.show()
df.printSchema()
display(df)

# ---------------------------------------------


# different ways to read CSV file data into data frames:

# 1st method:
df1 = spark.read.csv('dbfs:/FileStore/shared_uploads/nityaveera6@gmail.com/Order.csv', header=True)
df1.show(4)

# 2nd method:
df2 = spark.read.format(source='csv').option(key='header', value=True).load(path='dbfs:/FileStore/shared_uploads/nityaveera6@gmail.com/Order.csv')
df2.show(5)

# 3rd method:
df3 = spark.read.format(source='csv').options(header=True).load('dbfs:/FileStore/shared_uploads/nityaveera6@gmail.com/Order.csv')
df3.show(4)

# 4th method:
df = spark.read.csv(['dbfs:/FileStore/shared_uploads/Temp_Practice1/Read_Data/order1.csv', 'dbfs:/FileStore/shared_uploads/Temp_Practice1/Order.csv'], header=True)
df.show()

# ---------------------------------------------

# different ways to read JSON file data into data frames:

# 1st method:
df = spark.read.json('dbfs:/FileStore/shared_uploads/Temp/read/*.json')
df.show()

# 2nd method:
df2 = spark.read.format(source='json').option(key='header', value=True).load(path='dbfs:/FileStore/shared_uploads/Temp/read/*.json')
df2.show()

# 3rd method:
df3 = spark.read.format(source='json').options(header=True).load(path='dbfs:/FileStore/shared_uploads/Temp/read/*.json')
df3.show(4)


# ---------------------------------------------

# different modes to write CSV file data into data frames:

df.write.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True, mode='overwrite')
df.write.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True, mode='error')
df.write.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True, mode='ignore')
df.write.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True, mode='append')

display(spark.read.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True))

df1 = spark.read.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/csv_order', header=True)
# df1.show()
display(df1)


# ---------------------------------------------

# different modes to write JSON file data into data frames:

df.write.json(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order', mode='overwrite')
df.write.json(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order', mode='error')
df.write.json(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order', mode='ignore')
df.write.json(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order', mode='append')

display(spark.read.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order'))

df1 = spark.read.csv(path='dbfs:/FileStore/shared_uploads/Temp_Practice1/write/json_order')
df1.show()
display(df1)


# ---------------------------------------------


